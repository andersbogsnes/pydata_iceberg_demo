{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cd2acf2-77b9-43e9-85b6-d34508366e5e",
   "metadata": {},
   "source": [
    "# The History of the Data Lake\n",
    "\n",
    "There are a million implementations of the distributed file share, but the idea really took off with the publication of a Google paper entitled [\"The Google File System\"](https://research.google/pubs/the-google-file-system/) - a description of how Google had implemented their fault-tolerant, distributed file system, with data redundancy running on cheap consumer hardware while simultaneously being able to serve a Google-scale number of clients. This paper, combined with another Google paper on [MapReduce](https://research.google/pubs/mapreduce-simplified-data-processing-on-large-clusters/) laid out a programming model for effectively working with distributed data and served as the foundation for Hadoop, born in 2006 out of Yahoo.\n",
    "\n",
    "## In the Beginning, There was Hadoop\n",
    "\n",
    "![Hadoop logo](images/logos/hadoop_logo.png)\n",
    "\n",
    "Hadoop was actually an ecosystem, built around the ideas presented in the aforementioned Google papers. \n",
    "\n",
    "MapReduce, while key to the computational part of Hadoop, turned out to be fairly tricky to write, so a number of Apache projects sprung up around it. With the benefit of hindsight, the most significant of these would be Apache Hive, and Apache Spark. \n",
    "\n",
    "Hive provided a database-like SQL abstraction on top of MapReduce, while Apache Spark moved operations to memory, building a Directed Acyclic Graph (DAG) of operations to be performed on in-memory RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "## The Advent of the Data Lake\n",
    "\n",
    "![Data Lake](images/logos/datalake.png)\n",
    "\n",
    "The concept of a Data Lake was first coined in 2011 by the CTO of Pentaho to better contrast with the concept of a Data Mart. To him, the Data Mart was a targeted set of tables around curated data, but the promise of Hadoop's ecosystem was to be able to store the raw data directly, avoiding having to do the up-front work of deciding what was important, as well as being able to work with heterogeneous data. This resonated with the massive growth in data, where the reigning philosophy was to store it, just in case.\n",
    "\n",
    "## The Rise of AWS\n",
    "\n",
    "![AWS Savior](images/aws_savior.png)\n",
    "\n",
    "As many companies soon found out, actually running Hadoop was a pain, as maintaining the Hadoop HDFS systems alongside all the various distributed server technologies needed to be able to query the data was the domain of highly skilled (and expensive) engineers. AWS launched its Seriously Simple Storage (S3) in 2006, allowing companies to offload their Hadoop implementations onto S3 which became the standard for object storage. There was much rejoicing.\n",
    "\n",
    "## The Importance of File Formats\n",
    "\n",
    "Each iteration of distributed file shares have given us better ways of managing the files in a multi-client, fault-tolerant manner. Allowing us to store petabytes of data in files means that the file formats themselves become a key factor in maximizing query performance. \n",
    "\n",
    "Let's walk through the most common file formats used in modern Data Engineering\n",
    "\n",
    "# The three wise row-oriented file formats\n",
    "\n",
    "## The CSV\n",
    "\n",
    "![CSV Logo](images/logos/csv_logo.png)\n",
    "\n",
    "The CSV is the workhorse of Data Engineering, predating Personal Computers by over a decade. Everyone understands CSV and pretty much every system can generate CSVs.\n",
    "\n",
    "Plain text, human-readable, even Jupyter can read CSV, what's not to love?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be18a9-bd96-4b28-bfc2-d1999c599fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e043cf-18bc-454a-9591-31305afb1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('data/10.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3a4cc-0adf-4c86-a319-7e03db83e7d4",
   "metadata": {},
   "source": [
    "While easy to read, since a CSV is just text, each column's datatype has been inferred, through a `CSVSniffer` - by defaults sampling the first 100 rows and guessing at the correct datatype. \n",
    "\n",
    "If that sounds error-prone, that's because it is!\n",
    "\n",
    "Of course the worst offender is that there is no standard for CSV files (well, technically there's [RFC 4180](https://datatracker.ietf.org/doc/html/rfc4180) but no-one seems to care) - if you look at any CSV parsing library or function, they are forced to handle any number of potential formats. `polars.read_csv` has 33 arguments, `pandas.read_csv` has 49. This makes portability of CSV difficult, as there's a lot of edge cases to handle across systems. The other important aspect of CSVs to understand is that they are *row-oriented*. That was briefly mentioned previously, so let's dive into what that means:\n",
    "\n",
    "Given this data:\n",
    "\n",
    "![Example Data](images/columnar_vs_row.png)\n",
    "\n",
    "A CSV file would look like this to the parser:\n",
    "`Seller,Product,Sales ($)\\James,Shoes,20.00\\Kirk,Shoes,27.50\\nPicard,Socks,5.00`\n",
    "\n",
    "If I want to sum up all the sales, the scanner needs to read through each character one-by-one to identify the `,` separator which signifies a column and `\\n` which signifies a row.\n",
    "\n",
    "![CSV Parser](images/csv_reader.png)\n",
    "\n",
    "Then it would throw out 2/3rds of the data it read into memory and finally convert the `Sales ($)` string into floats and do the sum.\n",
    "\n",
    "It remains a fact of Data Engineering life that you'll have to deal with CSVs, and luckily a lot of engineering effort has gone into building very performant csv readers that can automatically handle lots of CSV oddities. That doesn't mean we should accept CSVs - there's a rich suite of superior alternatives!\n",
    "\n",
    "## The JSON file\n",
    "\n",
    "![JSON logo](images/logos/json_logo.jpg)\n",
    "\n",
    "A step up from CSV, JSON, which was designed in 2001, was formalized in 2013 into the [ECMA standard](https://ecma-international.org/publications-and-standards/standards/ecma-404/), making it much more portable. JSON has simple datatypes, and each row can be processed independently, since the metadata is present in every row. It comes at the cost of verbosity though, as each key is repeated for each line, and the format is still row-based.\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"Seller\": \"James\", \"Product\": \"Shoes\", \"Sales\": 20.00}, \n",
    "    {\"Seller\": \"Kirk\", \"Product\": \"Shoes\", \"Sales\": 27.50}, \n",
    "    {\"Seller\": \"Picard\", \"Product\": \"Socks\", \"Sales\": 5.00}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276080b8-7e37-4d7a-adfe-9a540d2c30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates the json file for demo purposes\n",
    "# df.filter(pl.col(\"recommendationid\").is_not_null()).write_json(\"data/10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072903bc-5a04-47cd-83d9-5740b09ede12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jq '.[0]' data/10.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168fd50-d07c-4c78-94cd-80c3b5034ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_json('data/10.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5bbab-0db3-450a-88f2-4fe1127ff8c3",
   "metadata": {},
   "source": [
    "## Apache Avro\n",
    "\n",
    "![Apache Avro Logo](images/logos/avro_logo.png)\n",
    "\n",
    "Avro compared to its siblings is a youngster, having joined the Apache Hadoop project in 2009, and is used mainly as a data interchange format, much like JSON, but is a binary format with a schema defined in JSON. Avro is a row-oriented format, and is a common format used in message brokers like Kafka.\n",
    "\n",
    "An Avro schema is defined as JSON and would look something like this - Avro introduces an upgrade to a rich type system, at the cost of human readability\n",
    "```json\n",
    "{\"namespace\": \"acme.avro\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"Sales\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"Seller\", \"type\": [\"string\", \"null\"]},\n",
    "     {\"name\": \"Product\",  \"type\": \"string\"},\n",
    "     {\"name\": \"Sales\", \"type\": \"float\"}\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "The data is then encoded into the Avro binary format based on the schema, and the consumer would use the schema to decode the incoming binary data.\n",
    "While used in the Hadoop ecosystem to transmit data back and forth between nodes, Avro is not commonly seen as the format used to store the actual data in a Data Lake, serving more as an excellent way to store metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca3add-9c9f-452c-9cf6-16563b0c3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro.schema\n",
    "from avro.datafile import DataFileWriter\n",
    "from avro.io import DatumWriter\n",
    "import json\n",
    "\n",
    "avro_schema = {\"namespace\": \"reviews.avro\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"Review\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"recommendationid\", \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": \"language\",  \"type\": \"string\", \"logicalType\": \"time-millis\"},\n",
    "     {\"name\": \"timestamp_created\", \"type\": \"int\", \"logicalType\": \"time-millis\"},\n",
    "     {\"name\": \"timestamp_updated\", \"type\": \"int\", \"logicalType\": \"time-millis\"},\n",
    "     {\"name\": 'voted_up', \"type\": \"int\"},\n",
    "     {\"name\": 'votes_up', \"type\": \"long\"},\n",
    "     {\"name\": 'votes_funny', \"type\": \"long\"},\n",
    "     {\"name\": 'weighted_vote_score', \"type\": \"float\"},\n",
    "     {\"name\": 'comment_count', \"type\": \"long\"},\n",
    "     {\"name\": 'steam_purchase', \"type\": \"int\"},\n",
    "     {\"name\": 'received_for_free',\"type\": \"int\"},\n",
    "     {\"name\": 'written_during_early_access', \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": 'hidden_in_steam_china', \"type\": \"long\"},\n",
    "     {\"name\": 'steam_china_location', \"type\": [\"string\", \"null\"]},\n",
    "     {\"name\": 'author_steamid', \"type\": \"long\"},\n",
    "     {\"name\": 'author_num_games_owned', \"type\": \"int\"},\n",
    "     {\"name\": 'author_num_reviews', \"type\": \"int\"},\n",
    "     {\"name\": 'author_playtime_forever', \"type\": \"int\"},\n",
    "     {\"name\": 'author_playtime_last_two_weeks', \"type\": \"int\"},\n",
    "     {\"name\": 'author_playtime_at_review', \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": 'author_last_played', \"type\": \"int\", \"logicalType\": \"time-millis\"}\n",
    " ]\n",
    "}\n",
    "\n",
    "reviews_schema = avro.schema.parse(json.dumps(avro_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7e7cc-feaa-44c2-9733-867dc2fe1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/10.avro\", \"wb\") as f:\n",
    "    writer = DataFileWriter(f, DatumWriter(), reviews_schema)\n",
    "    for record in df.filter(pl.col(\"recommendationid\").is_not_null()).to_dicts():\n",
    "        writer.append(record)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f26eb-c7bd-4b03-b55f-c4cea103d772",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!ls -lh ./data | awk '{print $5, $9}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd0ee7-caa9-4616-b43f-744b957caeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_avro('data/10.avro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0168d13-4b28-4465-a8ae-78cdf81aa21f",
   "metadata": {},
   "source": [
    "# The Angels of Column-Oriented File Formats\n",
    "## The First Herald\n",
    "\n",
    "![Apache ORC Logo](images/logos/apache_orc_logo.png)\n",
    "\n",
    "Initially released in 2013, ORC was developed by Hortonworks, a now-defunct provider of Hadoop-as-a-platform, and Facebook who have been heavily invested in the Hadoop ecosystem to handle its analytical needs. It was the successor to the RCFile format that was previously used in Hive.\n",
    "\n",
    "ORC is our first example of a columnar-based dataformat - a typed binary format that is stored in columns, allowing for easy access to a given column of data.\n",
    "\n",
    "![Column-oriented storage](images/column_storage.png)\n",
    "\n",
    "Now we can leverage metadata to skip reading large parts of the file that we don't need, and the binary nature means we should get small files\n",
    "\n",
    "ORC is closely linked to the Hive ecosystem, and is commonly seen in organizations that invested heavily in Hive, such as Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84476b39-c26e-46a0-a6bf-18867c15f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import orc\n",
    "# Known issue with all-null columns\n",
    "orc.write_table(df.select(pl.all().exclude('steam_china_location')).to_arrow(), \"data/10.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb6e10-022b-4ee6-b77f-e36a3230af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh ./data | awk '{print $5, $9}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30bdec-931a-491b-bb9f-3e758aaa6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_arrow(orc.read_table('data/10.orc', columns=['language', 'votes_up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145a530-c17b-4dd4-ab5f-448321699b68",
   "metadata": {},
   "source": [
    "## The Savior\n",
    "\n",
    "![Apache Parquet Logo](images/logos/Apache_Parquet_logo.png)\n",
    "\n",
    "While only a month older than it's spiritual twin, Parquet has become the defacto standard of the datalake. Parquet was created by Twitter and Cloudera in 2013 to handle it's Hadoop needs, and was based on another Google paper describing the [Dremel](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf) query system.\n",
    "\n",
    "Generally considered more language-agnostic, Parquet has become the default choice outside of Hive implementations and is generally the go-to format when working in Python.\n",
    "\n",
    "While Parquet and ORC are generally considered column-oriented data formats, this is actually not true - they are hybrid formats, combining the strengths of row-and-column orientation through striping (ORC term) or row groups (Parquet term). A Row Group will contain a set of grouped data along with metadata describing statistics of that data. This is a key detail in their implementation as this allows query engines to skip parts of the file that aren't relevant to the query, through reading the metadata to get information such as number of rows, columns, min, max etc. depending on the writing engine.\n",
    "\n",
    "The name of the game is to skip files - the most expensive part of any query is opening a file for reading. At scale, anything that lets us skip reading files will be key to performance.\n",
    "\n",
    "![Parquet Architecture](images/parquet_format.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14932b5-ddc7-4692-92a3-4ab3e7bbac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(\"data/10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a86b0-23fd-41c5-ae3e-13fffc946214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(\"data/10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293b2fd-4350-48ca-8606-1cbc49ebdc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh ./data | awk '{print $5, $9}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190e913-2bf6-48c1-8ba0-81caf6519e28",
   "metadata": {},
   "source": [
    "## The Holy Spirit\n",
    "\n",
    "![Apache Arrow Logo](images/logos/apache_arrow_logo.svg)\n",
    "\n",
    "Apache Arrow is an in-memory data format specification. While this means that it's not a file format, it's a key player in the data landscape, as it specifies a shared memory format for tools to adopt. This means that tools can perform zero-copy conversions between representations, as long as they can understand the Arrow specification. \n",
    "\n",
    "In the Python ecosystem, many tools have moved towards adopting Arrow as the native memory format. This includes Pandas, Polars, DuckDB and a whole host of other libraries. The ecosystem also contains tools such as Arrow Flight, an RPC protocol for exchanging client-server via Arrow, Arrow FlightSQL as a server specification for SQL, as well as the Arrow Database Connectivity (ADBC) which aims to provide a client-side abstraction on top.\n",
    "\n",
    "In short, Arrow is the Lingua Franca of exchanging data, and many of the examples in this notebook are driven by the `pyarrow` library, which is the Python reference implementation based on C++ bindings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0028035-cf93-47ef-a2d1-477808d9943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fac5b-179c-43a1-abf4-19db6cb3e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pq.read_table('data/10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9992fe7-76d0-4329-b353-1a272c1d3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
