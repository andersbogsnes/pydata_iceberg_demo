{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28216506-88b3-4a67-a919-edcb56118425",
   "metadata": {},
   "source": [
    "# BONUS: Duckdb\n",
    "\n",
    "![DuckDB Logo](images/logos/DuckDB_Logo.png)\n",
    "\n",
    "DuckDB is the new black in data engineering - an in-process analytical database that aims for performance. DuckDB is focused on making it easy to query data from anywhere and has bindings to most popular languages, including Python of course. It even compiles to WASM, letting us do cool stuff like [this](https://shell.duckdb.org/)\n",
    "\n",
    "DuckDB takes advantage of Arrow as it's internal data format, making it easy to interop with popular Python libraries as DuckDB can read and write the Arrow memory directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e6c8c-6870-40f1-bb3c-a26e4ec22e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "pl.Config().set_thousands_separator(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edf689-b3d1-4488-bf1d-1c80fb881cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"SELECT * FROM 'data/10.csv' WHERE language = 'english'\"\"\"\n",
    "\n",
    "duckdb.sql(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6df2d-3345-476a-b4fa-c93d6dd42898",
   "metadata": {},
   "source": [
    "DuckDB infers that we want to read a 'csv' file and calls it's `read_csv` function implicitly. We can of course do this explicitly if we want to pass options to handle those messy CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5b09b-bf8c-49b6-b0c1-b2e07f8b8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"SELECT filename, * FROM read_csv('data/10.csv', filename = true) WHERE language = 'english'\"\"\"\n",
    "my_polars_df = duckdb.sql(sql).pl()\n",
    "my_polars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6df69-ae64-4ba9-af3c-652fea10dee3",
   "metadata": {},
   "source": [
    "Because DuckDB is both in-process, as well as Arrow-backed, it's able to easily interop with other analytical tools, such as `polars` and `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde701d0-600a-4638-b148-738f7b1126a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CAST(received_for_free as bool) as received_for_free, \n",
    "AVG(votes_up) as num_upvotes  \n",
    "FROM my_polars_df \n",
    "GROUP BY ALL\n",
    "\"\"\"\n",
    "duckdb.execute(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3d158-02f7-49ec-af92-63a54e17e633",
   "metadata": {},
   "source": [
    "This works with Parquet as well, while supporting using the Parquet metadata to filter data, allowing us to process larger-than-RAM data easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9cc0c-29b1-4d95-83a8-41bea2123e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT language, count() as num_languages\n",
    "FROM 'data/parquet/all_reviews.parquet' \n",
    "GROUP BY ALL\n",
    "ORDER BY num_languages DESC\n",
    "\"\"\"\n",
    "duckdb.sql(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428eff36-28a1-40ee-8343-45886c5ddf2e",
   "metadata": {},
   "source": [
    "## Reading remote data\n",
    "A killer feature is the nativeness of reading data from object stores directly, including common data lake formats such as Parquet. It can even query MySQL and Postgres!\n",
    "\n",
    "Duckdb comes with a built-in secrets manager to handle credentials for connecting to remote stores so lets set that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15de14-0cd7-4ce9-bc67-d418761e859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.execute(\"\"\"CREATE OR REPLACE SECRET minio (\n",
    "    TYPE S3,\n",
    "    KEY_ID 'minio',\n",
    "    SECRET 'minio1234',\n",
    "    ENDPOINT 'minio:9000',\n",
    "    URL_STYLE 'path',\n",
    "    USE_SSL false,\n",
    "    REGION 'us-east-1'\n",
    ")\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4d586-5f3a-4220-83e7-3469d5b259c6",
   "metadata": {},
   "source": [
    "Secrets can be stored persistently or in-memory - here we persist in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92cd54-a3a3-4758-9ffa-35fd1e0f2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.execute(\"FROM duckdb_secrets()\").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338182d-5c9a-41f3-8947-d7ae01440a99",
   "metadata": {},
   "source": [
    "With credentials in order, we can treat S3 as just another file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d147d0-1331-40fe-8298-765cb63bfde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"FROM 's3://datalake/extract/reviews/10.csv'\"\n",
    "\n",
    "duckdb.execute(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ec412-7a89-4a9b-8104-566fb479c0ef",
   "metadata": {},
   "source": [
    "Since DuckDB can both read and write from remote locations in a number of file formats, it's a great swiss army knife for ETL - let's build a tiny pipeline to clean up the review data and convert to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2589504-a4af-4b38-984c-8b586d35bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"COPY (SELECT * FROM 's3://datalake/extract/reviews/10.csv' WHERE recommendationid is not null) TO 's3://datalake/extract/duckdb/10.parquet' (FORMAT PARQUET)\"\n",
    "duckdb.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a6231-e6d4-45c4-b610-c4014108745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT language, COUNT() as num_languages FROM 's3://datalake/extract/duckdb/10.parquet' GROUP BY ALL ORDER BY num_languages DESC\"\n",
    "duckdb.sql(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79434785-ce23-44f4-8d08-6b4d80f09ff6",
   "metadata": {},
   "source": [
    "DuckDB will intelligently use the S3 `Range` header to fetch only the data that is required from the Parquet file, unlike the CSV where we need to read the whole CSV file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb2a50-545c-4cd7-9801-a40075955910",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT language, COUNT() as num_rows \n",
    "FROM 's3://datalake/extract/duckdb/all_reviews.parquet' \n",
    "GROUP BY ALL \n",
    "ORDER BY num_rows DESC\n",
    "\"\"\"\n",
    "duckdb.sql(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89948829-9612-43f3-8b9e-dd5b85c3fec2",
   "metadata": {},
   "source": [
    "We can also parse multiple files using a glob - very handy for folders of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6228dd8-ee97-4354-b5e2-8d423c836c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT filename.parse_filename(true) as game_id, * EXCLUDE filename\n",
    "FROM read_csv('s3://datalake/extract/reviews/*.csv', filename = true)\n",
    "WHERE recommendationid is not null\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "duckdb.execute(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05737541-f7f5-4284-a37c-6435d05b3b3d",
   "metadata": {},
   "source": [
    "### Iceberg\n",
    "Can we do this with Iceberg? Of course! Let's use the AWS data from before to show off a more common usecase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9c40f-fd0d-4311-be6b-67dc99a1dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"CREATE OR REPLACE SECRET pydata (\n",
    "    TYPE S3,\n",
    "    PROVIDER CREDENTIAL_CHAIN,\n",
    "    SCOPE 's3://pydata-copenhagen-datalake'\n",
    ")\n",
    "\"\"\"\n",
    "duckdb.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d7f07-3375-4c66-8b6b-9e191f122595",
   "metadata": {},
   "source": [
    "DuckDB needs to know what Metadata file is the most current one, so we can use our Iceberg Catalog to get that information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b41953-1141-40d7-93fe-ffba1921afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872e9a8-ebac-4163-8718-dc0b1b834bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"aws_iceberg\", **{\"type\": \"glue\", \"glue.region\": \"eu-north-1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3304569-3b48-4575-a5ea-fdd11ddb7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"steam.reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c1c84-7a1e-47b4-92a3-3a2ca31ee807",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.metadata_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f02db7-a0d7-493a-9260-effedc7aefd7",
   "metadata": {},
   "source": [
    "DuckDB is extendible via `extensions`. We have actually been using the `httpfs` extension to talk to S3, but that one is auto-loaded when used since it's a common one to use. The Iceberg extension needs to be explicitly installed and loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc1dbe-313a-4db1-9307-0e45cb323844",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.install_extension('iceberg')\n",
    "duckdb.load_extension('iceberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520659d-baff-4722-a53f-27a591a96175",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"SELECT COUNT() as num_reviews FROM iceberg_scan('{table.metadata_location}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac4ea3-329e-400a-9272-7ef5f2025925",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72eedea-6d83-459c-9151-ab5184e59c0e",
   "metadata": {},
   "source": [
    "### Taking it up a notch\n",
    "Let's do something slightly more complicated - finding what language a given game_id has the most reviews in. We will also combine local and external data by fetching a mapping of game_id to name from the Steam API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a2083-6cdd-45e7-ae2a-4c87bcb13451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"CREATE TABLE apps as (SELECT unnest(applist.apps, recursive := true) FROM read_json('https://api.steampowered.com/ISteamApps/GetAppList/v2'))\"\n",
    "duckdb.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef2a6d-a959-449b-b57b-f6c2dca3a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "with ranked_reviews as (\n",
    "SELECT \n",
    "    game_id, \n",
    "    language, \n",
    "    count() as num_reviews,\n",
    "    row_number() OVER (PARTITION BY game_id ORDER BY num_reviews DESC) as rank\n",
    "    FROM iceberg_scan('{table.metadata_location}')\n",
    "    GROUP BY game_id, language\n",
    "    QUALIFY\n",
    "        rank = 1\n",
    ")\n",
    "SELECT \n",
    "apps.name, \n",
    "language, \n",
    "num_reviews \n",
    "FROM ranked_reviews\n",
    "JOIN apps on ranked_reviews.game_id = apps.appid\n",
    "ORDER BY num_reviews DESC\n",
    "\"\"\"\n",
    "duckdb.sql(sql).pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc4cb5-9fb2-4280-be3d-98e21b81d532",
   "metadata": {},
   "source": [
    "This is such a common pattern, that pyiceberg by default has a `to_duckdb` method. Note, that this will download the data locally to work with in memory, so we would no longer be pushing queries to the storage backend - large datasets need to be filtered first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adb194-8e66-4ab9-b84b-907098ac7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = table.scan(selected_fields=[\"language\"]).to_duckdb(table_name='languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a0f51-b577-4638-9908-c7c00d3529f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.sql(\"\"\"\n",
    "SELECT language, count() as num_languages \n",
    "FROM languages \n",
    "GROUP BY ALL\n",
    "ORDER BY num_languages DESC\"\"\").pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847a8be-8fbd-4be1-b1f5-f5d972d140fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
